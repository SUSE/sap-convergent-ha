.\" Version: 0.1
.\"
.TH ocf_suse_SAPCMControlZone 7 "18 Mar 2024" "" "SAPCMControlZone"
.\"
.SH NAME
.\"
SAPCMControlZone \- Manages Convergent Mediation ControlZone platform services for a single instance as HA resource.
.PP
.\"
.SH SYNOPSYS
.\"
\fBSAPCMControlZone\fP [ start | stop | monitor | meta\-data | methods | reload | usage | validate\-all ]
.PP
.\"
.SH DESCRIPTION
.\"
SAPCMControlZone is a resource agent for managing the Convergent Mediation (CM)
ControlZone platform and UI for a single instance as HA resources. 
.PP
The CM central ControlZone platform is responsible for providing services to
other instances. Several platform containers may exist in a CM system, for high
availability, but only one is active at a time.
.\" see https://infozone.atlassian.net/wiki/spaces/MD9/pages/4863840/Terminology
The CM central ControlZone UI is used to query, edit, import, and export data.
.\" see https://infozone.atlassian.net/wiki/spaces/MD83/pages/5966420/3.+Web+UI
This ControlZone services can be handled as active/passive resources.
.PP
NFS shares with work directories can be mounted statically on all nodes. The
HA cluster does not need to control that filesystems.
.PP
The resource agent uses the following interface provided by the ControlZone
command shell:
.PP
mzsh startup \fISERVICE\fP
.PP
mzsh status
.PP
mzsh shutdown \fISERVICE\fP
.PP
mzsh kill \fISERVICE\fP
.PP
Currently supported services are "platform" and "ui".
.\" TODO output
.PP
Please see also the REQUIREMENTS section below.
.PP
.\"
.SH SUPPORTED PARAMETERS
.\"
This resource agent supports the following parameters:
.PP
\fBUSER\fP
.RS 4
OS user who calls mzsh, owner of $MZ_HOME.
.br
Optional. Unique, string. Default value: "mzadmin".
.RE
.PP
\fBSERVICE\fP
.RS 4
The ControlZone service to be managed by the resoure agent.
.br
Optional. Unique, [ platform | ui ]. Default value: "platform".
.RE
.PP
\fBMZSHELL\fP
.RS 4
Path to mzsh. Could be one or two full paths. If one path is given, that path is
used for all actions. In case two paths are given, the first one is used for monitor
actions, the second one is used for start/stop/kill actions. If two paths are given,
the first needs to be on local disk, the second needs to be on the central NFS share
with the original CM ControlZone installation. Two paths are separated by a colon (:).
The mzsh contains settings that need to be consistent with MZ_HOME and JAVA_HOME.
.br
.\" TODO default /opt/cm9/bin/mzsh ?
Optional. Unique, string. Default value: "/usr/bin/mzsh".
.RE
.PP
\fBMZ_HOME\fP
.RS 4
Path to Convergent Mediation installation dirctory, owned by the mz-user.
Could be one or two full paths. If one path is given, that path is
used for all actions. In case two paths are given, the first one is used for monitor
actions, the second one is used for start/stop/kill actions. If two paths are given,
the first needs to be on local disk, the second needs to be on the central NFS share
with the original CM ControlZone installation. Two paths are separated by a colon (:). 
(Not yet implemented.)
.br
.\" TODO default /opt/cm9/ ?
Optional. Unique, string. Default value: "/opt/cm/".
.RE
.PP
\fBJAVA_HOME\fP
.RS 4
Path to Java virtual machine used by mzsh.
Could be one or two full paths. If one path is given, that path is
used for all actions. In case two paths are given, the first one is used for monitor
actions, the second one is used for start/stop/kill actions. If two paths are given,
the first needs to be on local disk, the second needs to be on the central NFS share
with the original CM ControlZone installation. Two paths are separated by a colon (:). 
(Not yet implemented.)
.br
.\" TODO default /opt/cm9/sapmachine17 or OS $JAVA_HOME ?
Optional. Unique, string. Default value: "/usr/lib64/jvm/jre-17-openjdk".
.RE
.PP
\fBCALL_TIMEOUT\fP
.RS 4
Define timeout how long calls to the ControlZone platform for checking the
status can take. If the timeout is reached, the return code will be 124. If you
increase this timeout for ControlZone calls, you should also adjust the monitor
operation timeout of your Linux cluster resources. (Not yet implemented.)
.br
Optional. Unique, integer. Default value: 60.
.RE
.PP
\fBSHUTDOWN_RETRIES\fP
.RS 4
Number of retries to check for process shutdown. Passed to mzsh.
If you increase the number of shutdown retries, you should also adjust the stop
operation timeout of your Linux cluster resources. (Not yet implemented.)
.br
Optional. Unique, integer. Default: mzsh builtin value.
.RE
.PP
.\" \fBVERBOSE_STATUS\fP
.\" .RS 4
.\" Enables verbose mode. Passed to mzsh. (Not yet implemented.)
.\" .br
.\" Optional. Unique, [ yes | no ]. Default value: no.
.\" .RE
.\" .PP
.\"
.SH SUPPORTED ACTIONS
.\"
This resource agent supports the following actions (operations):
.PP
\fBstart\fR
.RS 4
Starts the ControlZone platform resource.
.br
Timeout might be adapted to match expected application timing.
Suggested minimum timeout: 120\&.
.RE
.PP
\fBstop\fR
.RS 4
Stops the ControlZone platform resource.
Timeout might be adapted to match expected application timing.
Suggested minimum timeout: 120\&.
.RE
.PP
\fBmonitor\fR
.RS 4
Regularly checks the ControlZone platform resource status.
Timeout might be adapted to be greater than expected infrastructure timeouts.
Suggested minimum timeout: 120, suggested interval: 120,
suggested action on-fail=restart\&.
.RE
.PP
\fBvalidate\-all\fR
.RS 4
Performs a validation of the resource configuration. It does basic checking of
given USER, MZSHELL and SERVICE.
Suggested minimum timeout: 5\&.
.RE
.PP
\fBmeta\-data\fR
.RS 4
Retrieves resource agent metadata (internal use only).
Suggested minimum timeout: 5\&.
.RE
.PP
\fBmethods\fR
.RS 4
Reports which methods (operations) the resource agent supports.
Suggested minimum timeout: 5\&.
.RE
.PP
\fBreload\fR
.RS 4
Change parameters without forcing a recover of the resource.
Suggested minimum timeout: 5\&.
.RE
.PP
.\"
.SH RETURN CODES
.\"
The return codes are defined by the OCF cluster framework. Please refer to the
OCF definition on the website mentioned below. In addition return code 124 will 
be logged if CALL_TIMEOUT has been exceeded. Also log entries are written, which
can be scanned by using a pattern like "SAPCMControlZone.*RA.*rc=[1-7,9]" for
errors. Regular operations might be found with "SAPHanaControlZone.*RA.*rc=0".
See SUSE TID 7022678 for maximum RA tracing.
.RE
.PP
.\"
.SH EXAMPLES
.\"
Configuration and basic checks for ControlZone platform resources in Linux clusters.
See also man page SAPCMControlZone_maintenance_examples(7).
.PP
\fB* Example configuration for resource group with ControlZone platform and IP address.\fR
.PP
A ControlZone platform resoure rsc_cz_C11 is configured, handled by OS user
c11adm. The local /opt/cm9/c11/bin/mzsh is used for monitoring, but for other
actions /usr/sap/c11/bin/mzsh is used.
This resource is grouped with an IP address resource rsc_ip_C11 into
group grp_cz_C11. The IP address starts first. The resource group might run on
either node, but never in parallel.
.PP
In case of ControlZone platform failure (or monitor timeout), the resource
group gets restarted until it gains success or migration-threshold is reached.
If migration-threshold is exceeded, or if the node where the group runs fails,
the group will be moved to the other node.
A priority is configured for correct fencing in split-brain situations.
See also SAPCMControlZone_basic_cluster(7) and ocf_heartbeat_IPaddr2(7).
.PP
.RS 2
primitive rsc_cz_C11 ocf:suse:SAPCMControlZone \\
.br
 params USER=c11adm \\
.br
 MZSHELL=/opt/cm9/c11/bin/mzsh:/usr/sap/c11/bin/mzsh \\
.br
 MZ_HOME=/opt/cm9/c11/:/usr/sap/c11/ \\
.br
 JAVA_HOME=/opt/cm9/c11/sapmachine17:/usr/sap/c11/sapmachine17 \\
.br 
 op monitor interval=120 timeout=120 on-fail=restart \\
.br
 op start timeout=120 \\
.br
 op stop timeout=120 \\
.br
 meta priority=100
.RE
.PP
.RS 2
primitive rsc_ip_C11 ocf:heartbeat:IPaddr2 \\
.br
 params ip=192.168.1.234 \\
.br
 op monitor interval=60 timeout=20 on-fail=restart
.RE
.PP
.RS 2
group grp_cz_C11 \\
.br
 rsc_ip_C11 rsc_cz_C11
.PP
.RE
.PP
\fB* Example configuration for resource ControlZone UI.\fR
.PP
A ControlZone UI resoure rsc_ui_C11 is configured, handled by OS user c11adm.
The default path to mzsh on central NFS share is used, no local copies are used
(sub-optimal setup).
The resource might run on either node, but never in parallel.
In case of ControlZone UI failure (or monitor timeout), the resource gets
restarted until it gains success or migration-threshold is reached. If
migration-threshold is exceeded, or if the node where the resource fails, the
resource will be moved to the other node. 
See also SAPCMControlZone_basic_cluster(7) and ocf_heartbeat_IPaddr2(7).
.br
Note: This resource might be grouped with an IP address resource, like in the
above platform example.
.PP
.RS 2
primitive rsc_UI_C11 ocf:suse:SAPCMControlZone \\
.br
 params USER=c11adm SERVICE=ui \\
.br
 op monitor interval=120 timeout=120 on-fail=restart \\
.br
 op start timeout=120 \\
.br
 op stop timeout=120
.RE
.PP
\fB* Optional Filesystem resource for monitoring NFS shares.\fR
.PP
A shared filesystem migth be statically mounted by OS on both cluster nodes.
This filesystem holds work directories. It must not be confused with the
ControlZone application itself. Client-side write caching has to be disabled.
.PP
A Filesystem resource is configured for a bind-mount of the real NFS share.
This resource is grouped with the ControlZone platform and IP address. In case
of filesystem failures, the whole group gets restarted.
No mount or umount on the real NFS share is done.
Example for the real NFS share is /mnt/platform/check/, example for the
bind-mount is /mnt/check/. Both mount points have to be created before the
cluster resource is activated. 
See also man page SAPCMControlZone_basic_cluster(7), ocf_heartbeat_Filesystem(7)
and nfs(5).
.PP
.RS 2
primitive rsc_fs_C11 ocf:heartbeat:Filesystem \\
.br
 params device=/mnt/platform/check/ directory=/mnt/check/ \\
.br
 fstype=nfs4 options=bind,rw,noac,sync,defaults \\
.br
 op monitor interval=120 timeout=120 on-fail=restart \\
.br
 op_params OCF_CHECK_LEVEL=20 \\
.br
 op start timeout=120 \\
.br
 op stop timeout=120
.RE
.PP
.RS 2
group grp_cz_C11 \\
.br
 rsc_fs_C11 rsc_ip_C11 rsc_cz_C11
.RE
.PP
\fB* Show configuration of ControlZone platform resource and resource group.\fR
.PP
Resource is rsc_cz_C11, resource group is grp_C11.
.PP
.RS 2 
# crm configure show rsc_cz_C11 grp_C11
.RE
.PP
\fB* Search for log entries of SAPCMControlZone, show errors only.\fR
.PP
.RS 2
# grep "SAPCMControlZone.*RA.*rc=[1-7,9]" /var/log/messages
.RE
.PP
\fB* Show and delete failcount for resource.\fR
.PP
Resource is rsc_cz_C11, node is node22. Useful after a failure has been fixed,
and for testing.
.PP
.RS 2
# crm resource failcount rsc_cz_C11 show node22.
.br
# crm resource failcount rsc_cz_C11 delete node22.
.RE
.PP
\fB* Manually trigger a SAPCMControlZone probe action.\fR
.PP
.RS 2
# OCF_ROOT=/usr/lib/ocf/ \\
.br
OCF_RESKEY_CRM_meta_interval=0 \\
.br
/usr/lib/ocf/resource.d/suse/SAPCMControlZone monitor
.RE
.PP
\fB* Basic validation of SAPCMControlZone configuration.\fR
.PP
The USER, MZSHELL and SERVICE are looked up in the installed system.
.PP
.RS 2
# OCF_ROOT=/usr/lib/ocf/ \\
.br
OCF_RESKEY_CRM_meta_interval=0 \\
.br
/usr/lib/ocf/resource.d/suse/SAPCMControlZone validate\-all
.RE
.PP
\fB* Example for testing the SAPCMControlZone RA.\fR
.PP
The ControlZone platform will be terminated, while controlled by the
Linux cluster. This could be done as very basic testing of SAPCMControlZone RA
integration. Terminating ControlZone platform processes is dangerous. This test
should not be done on production systems. Example user is mzadmin.
.br
Note: Understand the impact before trying.
.PP
.RS 2
1. Check ControlZone and Linux cluster for clean and idle state.
.br
2. Terminate ControlZone platform processes.
.br
 # su - mzadmin -c "mzsh kill platform"
.br
3. Wait for the cluster to recover from resource failure.
.br
4. Clean up resource fail-count.
.br
5. Check ControlZone and Linux cluster for clean and idle state.
.RE
.PP
.\"
.SH FILES
.\"
.TP
/usr/lib/ocf/resource.d/suse/SAPCMControlZone
the resource agent
.TP
.\" TODO two paths
$MZ_HOME, e.g. /opt/cm/
the installation directory of a ControlZone service
.TP
$MZ_HOME/bin/mzsh
the default mzshell, used as API for managing ControlZone components
.TP
$MZ_HOME/tmp/
temporary files of a ControlZone service
.TP
$JAVA_HOME
the JAVA virtual machine, used by mzsh
.\" see https://infozone.atlassian.net/wiki/spaces/MD9/pages/4863840/Terminology
.PP
.\"
.SH REQUIREMENTS
.\"
* Convergent Mediation ControlZone version 9.0.0.0 or higher is installed and
configured on both cluster nodes. The software is installed once into a shared
NFS filesystem. Then binaries and configuration are copied into both cluster
nodes´ local filesystems. Finally the local configuration has to be adjusted.
Please refer to Convergent Mediation documentation for details.
.PP
* Only one ControlZone instance per Linux cluster.
.PP
* Linux shell of the mz-user (e.g. "mzadmin") is /bin/bash.
.PP
* Technical users and groups are defined locally in the Linux system. If users
are resolved by remote service, local caching is neccessary. Substitute user
(su) to the mz-user needs to work reliable and without customized actions or
messages.
.PP
* Name resolution for hostnames and virtual hostnames is crucial. Hostnames of
cluster nodes and services are resolved locally in the Linux system.
.PP
* Strict time synchronization between the cluster nodes, e.g. NTP. All nodes of
a cluster have configured the same timezone.
.PP
* Needed NFS shares (e.g. /usr/sap/<sid>) are mounted statically or by automounter. 
No client-side write caching. File locking might be configured for application
needs.
.PP
* The RA monitoring operations have to be active.
.PP
* RA runtime almost completely depends on call-outs to controlled resources,
OS and Linux cluster. The infrastructure needs to allow these call-outs to
return in time.
.PP
* The ControlZone application is not started/stopped by OS. Thus there is no
SystemV, systemd or cron job.
.PP
* As long as the ControlZone application is managed by the Linux cluster, the
application is not started/stopped/moved from outside. Thus no manual actions
are done. The Linux cluster does not prevent from administrative mistakes.
However, if the Linux cluster detects the application running at both sites in
parallel, it will stop both and restart one.
.PP
* Interface for the RA to the ControlZone platform is the command mzsh. The
mzsh should be accessed on the cluster nodes´ local filesystems.
The mzsh is called with the arguments startup, shutdown, status and kill. Its
output is parsed by the RA. Thus the command and its output needs to be stable.
.PP
* The mzsh is called on the active node with a defined interval for regular
resource monitor operations. It also is called on the active or passive node in
certain situations. Those calls might run in parallel.
.PP
.\"
.SH BUGS
.\"
In case of any problem, please use your favourite SAP support process to open a
request for the component BC-OP-LNX-SUSE.
.br
Please report feedback and suggestions to feedback@suse.com.
.PP
.\"
.SH SEE ALSO
.\"
\fBSAPCMControlZone_basic_cluster\fP(7),
\fBSAPCMControlZone_maintenance_examples\fP(7),
\fBocf_heartbeat_IPaddr2\fP(7) , \fBocf_heartbeat_Filesystem\fP(7) ,
\fBcrm\fP(8) , \fBcrm_mon\fP(8) ,
\fBnfs\fP(5) , \fBmount\fP(8) ,
.br
http://clusterlabs.org/doc/en-US/Pacemaker/1.1/html/Pacemaker_Explained/s-ocf-return-codes.html ,
.br
https://infozone.atlassian.net/wiki/spaces/MD9/pages/4881672/mzsh ,
.br
https://infozone.atlassian.net/wiki/spaces/MD9/pages/4849693/Setting+Environment+Variables+for+Platform ,
.br
https://documentation.suse.com/sbp/sap/ ,
.br
https://documentation.suse.com/#sle-ha ,
.br
https://www.suse.com/support/kb/doc/?id=000019138 ,
.br
https://www.suse.com/support/kb/doc/?id=000019514 ,
.br
https://www.suse.com/support/kb/doc/?id=000019722 ,
.br
https://launchpad.support.sap.com/#/notes/1552925 ,
.br
https://launchpad.support.sap.com/#/notes/3079845
.PP
.\"
.SH AUTHORS
.\"
F.Herschel, L.Pinne
.PP
.\"
.SH COPYRIGHT
.\"
(c) 2023-2024 SUSE LLC
.br
SAPCMControlZone comes with ABSOLUTELY NO WARRANTY.
.br
For details see the GNU General Public License at
http://www.gnu.org/licenses/gpl.html
.\"
