.\" Version: 0.1
.\"
.TH ocf_suse_SAPCMControlZone 7 "15 Apr 2024" "" "SAPCMControlZone"
.\"
.SH NAME
.\"
SAPCMControlZone \- Manages Convergent Mediation ControlZone platform services for a single instance as HA resource.
.PP
.\"
.SH SYNOPSYS
.\"
\fBSAPCMControlZone\fP [ start | stop | monitor | meta\-data | methods | reload | usage | validate\-all ]
.PP
.\"
.SH DESCRIPTION
.\"
\fBOverview\fP
.PP
SAPCMControlZone is a resource agent (RA) for managing the Convergent Mediation
(CM) ControlZone platform and UI for a single instance as HA resources. 
.PP
The CM central ControlZone platform is responsible for providing services to
other CM instances. Several platform containers may exist in a CM system, for high
availability, but only one is active at a time.
.\" see https://infozone.atlassian.net/wiki/spaces/MD9/pages/4863840/Terminology
The CM central ControlZone UI is used to query, edit, import, and export data.
.\" see https://infozone.atlassian.net/wiki/spaces/MD83/pages/5966420/3.+Web+UI
.PP
The SAPCMControlZone RA manages platform services as active/passive resources.
The RA relies on the mzsh command of ControlZone as interface.
This calls are used:
.PP
.RS 2
- mzsh startup -f \fISERVICE\fP
.br
- mzsh status \fISERVICE\fP
.br
- mzsh shutdown \fISERVICE\fP
.\" .br
.\" TODO needed? - mzsh kill \fISERVICE\fP
.RE
.PP
Currently supported services are "platform" and "ui".
.\" TODO output
Please see also the REQUIREMENTS section below.
.PP
\fBFilesystems\fP
.PP
NFS shares with work directories can be mounted statically on all nodes. The
HA cluster does not need to control that filesystems. See also manual page
SAPCMControlZone_basic_cluster(7).
.PP
The ControlZone software and Java runtime environment can be installed into a
central NFS share, or into the cluster node´s local filesystems, or both. Again,
the HA cluster does not need to control that filesystems. The SAPCMControlZone
RA offers three ways for managing ControlZone services:
.PP
.RS 2
- calling mzsh always from central shared NFS
.br
- calling mzsh always from cluster node´s local filesystem
.br
- calling mzsh startup/shutdown centrally, but mzsh status locally
.\" TODO kill better local?
.RE
.PP
Another option would be to install the ControlZone software into a cluster
managed filesystem on shared storage. This might fit for on-premise systems
backed by SAN storage infrastructure. From SAPCMControlZone RA´s perspective
this would look like node´s local filesystem. We do not discuss storage details
here.
.PP
\fBBest Practice\fP
.PP
* Use two independent corosync rings, at least one of them on bonded network.
Resulting in at least three physical links. Unicast is preferred.
.PP
* Use three Stonith Block Device (SBD), shared LUNs across all nodes on all sites.
Of course, together with hardware watchdog.
.PP
* Align all timeouts in the Linux cluster with the timeouts of the underlying
infrastructure - particuarly network and storage.
.PP
* Prefer cluster node´s local filesystem over NFS, whenever possible.
.PP
* Check the installation of OS and Linux cluster on all nodes before doing any
functional tests.
.PP
* Carefully define, perform, and document tests for all scenarios that should be
covered, as well as all maintenance procedures.
.PP
* Test ControlZone features without Linux cluster before doing the overall
cluster tests.
.PP
* Test basic Linux cluster features without ControlZone before doing the overall
cluster tests.
.PP
* Be patient. For detecting the overall ControlZone status, the Linux cluster
needs a certain amount of time, depending on the ControlZone services and the
configured intervals and timeouts.
.PP
* Before doing anything, always check for the Linux cluster's idle status,
left-over migration constraints, and resource failures as well as the
ControlZone status.
Please see also manual page SAPCMControlZone_maintenance_examples(7).
.PP
.\"
.SH SUPPORTED PARAMETERS
.\"
This resource agent supports the following parameters:
.PP
\fBUSER\fP
.RS 4
OS user who calls mzsh, owner of $MZ_HOME.
.br
Optional. Unique, string. Default value: "mzadmin".
.RE
.PP
\fBSERVICE\fP
.RS 4
The ControlZone service to be managed by the resoure agent.
.br
Optional. Unique, [ platform | ui ]. Default value: "platform".
.RE
.PP
\fBMZSHELL\fP
.RS 4
Path to mzsh. Could be one or two full paths. If one path is given, that path is
used for all actions. In case two paths are given, the first one is used for monitor
actions, the second one is used for start/stop/kill actions. If two paths are given,
the first needs to be on local disk, the second needs to be on the central NFS share
with the original CM ControlZone installation. Two paths are separated by a colon (:).
The mzsh contains settings that need to be consistent with MZ_HOME and JAVA_HOME.
Please refer to Convergent Mediation product documentation for details.
.br
.\" TODO default /opt/cm9/bin/mzsh ?
Optional. Unique, string. Default value: "/usr/bin/mzsh".
.RE
.PP
\fBMZHOME\fP
.RS 4
Path to CM ControlZone installation dirctory, owned by the mz-user.
Could be one or two full paths. If one path is given, that path is used for all 
actions. In case two paths are given, the first one is used for monitor actions,
the second one is used for start/stop/kill actions. If two paths are given, the
first needs to be on local disk, the second needs to be on the central NFS share
with the original CM ControlZone installation. Two paths are separated by a
colon (:).
.br
.\" TODO default /opt/cm9/ ?
Optional. Unique, string. Default value: "/opt/cm/".
.RE
.PP
\fBJAVAHOME\fP
.RS 4
Path to Java virtual machine used for CM ControlZone.
Could be one or two full paths. If one path is given, that path is used for all
actions. In case two paths are given, the first one is used for monitor actions,
the second one is used for start/stop/kill actions. If two paths are given, the
first needs to be on local disk, the second needs to be on the central NFS share
with the original CM ControlZone installation. Two paths are separated by a
colon (:).
.br
.\" TODO default /opt/cm9/sapmachine17 or OS $JAVA_HOME ?
Optional. Unique, string. Default value: "/usr/lib64/jvm/jre-17-openjdk".
.RE
.PP
\fBMZPLATFORM\fP
.RS 4
URL used by mzsh for connecting to CM ControlZone services.
Should usually not be changed. The service´s virtual hostname or virtual IP
address managed by the cluster must never be used here.
.br
Optional. Unique, string. Default value: "http://localhost:9000".
.RE
.PP
\fBCALL_TIMEOUT\fP
.RS 4
Define timeout how long calls to the ControlZone service for checking the
status can take. If the timeout is reached, the return code will be 124. If you
increase this timeout for ControlZone calls, you should also adjust the monitor
operation timeout of your Linux cluster resources.
(Not yet implemented)
.br
Optional. Unique, integer. Default value: 60.
.RE
.PP
\fBSHUTDOWN_RETRIES\fP
.RS 4
Number of retries to check for process shutdown. Passed to mzsh.
If you increase the number of shutdown retries, you should also adjust the stop
operation timeout of your Linux cluster resources.
(Not yet implemented)
.br
Optional. Unique, integer. Default: mzsh builtin value.
.RE
.PP
.\" \fBVERBOSE_STATUS\fP
.\" .RS 4
.\" Enables verbose mode. Passed to mzsh. (Not yet implemented)
.\" .br
.\" Optional. Unique, [ yes | no ]. Default value: no.
.\" .RE
.\" .PP
.\"
.SH SUPPORTED ACTIONS
.\"
This resource agent supports the following actions (operations):
.PP
\fBstart\fR
.RS 4
Starts the ControlZone platform resource.
If the start call fails, the RA tries twice.
Timeout might be adapted to match expected application timing.
Suggested minimum timeout: 120\&.
.RE
.PP
\fBstop\fR
.RS 4
Stops the ControlZone platform resource.
If the stop call fails, the RA tries twice.
Timeout might be adapted to match expected application timing.
The RA stop timeout related to mzsh TODO, which defaults to 180 seconds.
For maximum patience, the RA stop timeout would be TODO
Suggested minimum timeout: 120\&.
.RE
.PP
\fBmonitor\fR
.RS 4
Regularly checks the ControlZone platform resource status.
If the status call fails, the RA tries twice.
Timeout might be adapted to be greater than expected infrastructure timeouts.
The RA stop timeout also relates to the ControlZome components property
pico.rcp.timeout, which defaults to 60 seconds.
For maximum patience with the component, the RA stop timeout would be 140
(60+10+60+10). Suggested minimum timeout: 120, suggested interval: 120,
suggested action on-fail=restart\&.
.RE
.PP
\fBvalidate\-all\fR
.RS 4
Performs a validation of the resource configuration. It does basic checking of
given USER, MZSHELL and SERVICE.
Suggested minimum timeout: 5\&.
.RE
.PP
\fBmeta\-data\fR
.RS 4
Retrieves resource agent metadata (internal use only).
Suggested minimum timeout: 5\&.
.RE
.PP
\fBmethods\fR
.RS 4
Reports which methods (operations) the resource agent supports.
Suggested minimum timeout: 5\&.
.RE
.PP
\fBreload\fR
.RS 4
Change parameters without forcing a recover of the resource.
Suggested minimum timeout: 5\&.
.RE
.PP
.\"
.SH RETURN CODES
.\"
The return codes are defined by the OCF cluster framework. Please refer to the
OCF definition on the website mentioned below. In addition return code 124 will 
be logged if CALL_TIMEOUT has been exceeded. Also log entries are written, which
can be scanned by using a pattern like "SAPCMControlZone.*RA.*rc=[1-7,9]" for
errors. Regular operations might be found with "SAPHanaControlZone.*RA.*rc=0".
See SUSE TID 7022678 for maximum RA tracing.
.PP
The RA also logs mzsh return codes. For that codes, please look for the respective
functions at
https://infozone.atlassian.net/wiki/spaces/MD91/pages/23375910/Always+Available
.PP
.\"
.SH EXAMPLES
.\"
Configuration and basic checks for ControlZone platform resources in Linux clusters.
See also man page SAPCMControlZone_maintenance_examples(7).
.PP
\fB* Example .bashrc\fR
.PP
TODO
MZ_HOME and JAVA_HOME are inherited from RA
.PP
.RS 2
# MZ_PLATFORM, MZ_HOME, JAVA_HOME are set by HA RA
.br
export MZ_PLATFORM=${RA_MZ_PLATFORM:-"http://localhost:9000"}
.br
export MZ_HOME=${RA_MZ_HOME:-"/opt/cm9/c11"}
.br
export JAVA_HOME=${RA_JAVA_HOME:-"/opt/cm9/c11/sapmachine17"}
.RE
.PP
\fB* Example configuration for resource group with ControlZone platform and IP address.\fR
.PP
A ControlZone platform resoure rsc_cz_C11 is configured, handled by OS user
c11adm. The local /opt/cm9/c11/bin/mzsh is used for monitoring, but for other
actions /usr/sap/c11/bin/mzsh is used.
This resource is grouped with an IP address resource rsc_ip_C11 into
group grp_cz_C11. The IP address starts first. The resource group might run on
either node, but never in parallel.
.PP
In case of ControlZone platform failure (or monitor timeout), the resource
group gets restarted until it gains success or migration-threshold is reached.
If migration-threshold is exceeded, or if the node fails where the group is
running, the group will be moved to the other node.
A priority is configured for correct fencing in split-brain situations.
See also SAPCMControlZone_basic_cluster(7) and ocf_heartbeat_IPaddr2(7).
.PP
.RS 2
primitive rsc_cz_C11 ocf:suse:SAPCMControlZone \\
.br
 params USER=c11adm \\
.br
 MZSHELL=/opt/cm9/c11/bin/mzsh:/usr/sap/c11/bin/mzsh \\
.br
 MZHOME=/opt/cm9/c11/:/usr/sap/c11/ \\
.br
 JAVAHOME=/opt/cm9/c11/sapmachine17:/usr/sap/c11/sapmachine17 \\
.br 
 op monitor interval=120 timeout=120 on-fail=restart \\
.br
 op start timeout=120 \\
.br
 op stop timeout=120 \\
.br
 meta priority=100
.RE
.PP
.RS 2
primitive rsc_ip_C11 ocf:heartbeat:IPaddr2 \\
.br
 params ip=192.168.1.234 \\
.br
 op monitor interval=60 timeout=20 on-fail=restart
.RE
.PP
.RS 2
group grp_cz_C11 \\
.br
 rsc_ip_C11 rsc_cz_C11
.PP
.RE
.PP
\fB* Example configuration for resource ControlZone UI.\fR
.PP
A ControlZone UI resoure rsc_ui_C11 is configured, handled by OS user c11adm.
The default path to mzsh 
.\" TODO on central NFS share
is used
.\" TODO , no local copies are used (sub-optimal setup)
.
The resource might run on either node, but never in parallel.
In case of ControlZone UI failure (or monitor timeout), the resource gets
restarted until it gains success or migration-threshold is reached. If
migration-threshold is exceeded, or if the node fails where the resource is
running, the resource will be moved to the other node. 
The resource rsc_ui_C11 will start after resource group grp_cz_C11 and run on
the same node.
See also SAPCMControlZone_basic_cluster(7) and ocf_heartbeat_IPaddr2(7).
.PP
.RS 2
primitive rsc_ui_C11 ocf:suse:SAPCMControlZone \\
.br
 params USER=c11adm SERVICE=ui \\
.br
 op monitor interval=120 timeout=120 on-fail=restart \\
.br
 op start timeout=120 \\
.br
 op stop timeout=120
.PP
order ord_cz_first Mandatory: grp_cz_C11:start rsc_ui_C11:start
.PP
colocation col_with_cz 2000: rsc_ui_C11:Started grp_cz_C11:Started
.RE
.PP
Note: Instead of defining order and colocation, the resource rsc_ui_C11 might be
just added to the resource group grp_cz_C11. This may impact the platform in some
situations.
.PP
\fB* Optional Filesystem resource for monitoring NFS shares.\fR
.PP
A shared filesystem migth be statically mounted by OS on both cluster nodes.
This filesystem holds work directories. It must not be confused with the
ControlZone application itself. Client-side write caching has to be disabled.
.PP
A Filesystem resource is configured for a bind-mount of the real NFS share.
This resource is grouped with the ControlZone platform and IP address. In case
of filesystem failures, the node gets fenced.
No mount or umount on the real NFS share is done.
Example for the real NFS share is /mnt/platform/check/, example for the
bind-mount is /mnt/check/. Both mount points have to be created before the
cluster resource is activated.
See also man page SAPCMControlZone_basic_cluster(7), ocf_heartbeat_Filesystem(7)
and nfs(5).
.PP
.RS 2
primitive rsc_fs_C11 ocf:heartbeat:Filesystem \\
.br
 params device=/mnt/platform/check/ directory=/mnt/check/ \\
.br
 fstype=nfs4 options=bind,rw,noac,sync,defaults \\
.br
 op monitor interval=60 timeout=120 on-fail=fence \\
.br
 op_params OCF_CHECK_LEVEL=20 \\
.br
 op start timeout=120 \\
.br
 op stop timeout=120
.RE
.PP
.RS 2
group grp_cz_C11 \\
.br
 rsc_fs_C11 rsc_ip_C11 rsc_cz_C11
.RE
.PP
\fB* Show configuration of ControlZone platform resource and resource group.\fR
.PP
Resource is rsc_cz_C11, resource group is grp_C11.
.PP
.RS 2 
# crm configure show rsc_cz_C11 grp_C11
.RE
.PP
\fB* Search for log entries of SAPCMControlZone, show errors only.\fR
.PP
.RS 2
# grep "SAPCMControlZone.*rc=[1-7,9]" /var/log/messages
.RE
.PP
\fB* Show log entry of one specific SAPCMControlZone run.\fR
.PP
PID of run is 8558.
.PP
.RS 2
# grep "SAPCMControlZone.*\\[8558\\]" /var/log/messages
.RE
.PP
\fB* Show and delete failcount for resource.\fR
.PP
Resource is rsc_cz_C11, node is node22. Useful after a failure has been fixed,
and for testing.
.PP
.RS 2
# crm resource failcount rsc_cz_C11 show node22.
.br
# crm resource failcount rsc_cz_C11 delete node22.
.RE
.PP
\fB* Manually trigger a SAPCMControlZone probe action.\fR
.PP
USER is mzadmin, SERVICE is platform, MZSHELL is /usr/sap/c11/bin/mzsh .
.PP
.RS 2
# OCF_RESKEY_USER=mzadmin \\
.br
OCF_RESKEY_SERVICE=platform \\
.br
OCF_RESKEY_MZSHELL="/usr/sap/c11/bin/mzsh" \\
.br
OCF_RESKEY_MZHOME="/usr/sap/c11" \\
.br
OCF_RESKEY_JAVAHOME="/usr/sap/sapmachine17" \\
.br
OCF_ROOT=/usr/lib/ocf/ \\
.br
OCF_RESKEY_CRM_meta_interval=0 \\
.br
/usr/lib/ocf/resource.d/suse/SAPCMControlZone monitor
.RE
.PP
\fB* Basic validation of SAPCMControlZone configuration.\fR
.PP
The USER, MZSHELL and SERVICE are looked up in the installed system.
.PP
.RS 2
# OCF_ROOT=/usr/lib/ocf/ \\
.br
OCF_RESKEY_CRM_meta_interval=0 \\
.br
/usr/lib/ocf/resource.d/suse/SAPCMControlZone validate\-all
.RE
.PP
\fB* Example for testing the SAPCMControlZone RA.\fR
.PP
The ControlZone platform will be terminated, while controlled by the
Linux cluster. This could be done as very basic testing of SAPCMControlZone RA
integration. Terminating ControlZone platform processes is dangerous. This test
should not be done on production systems. Example user is mzadmin.
.br
Note: Understand the impact before trying.
.PP
.RS 2
1. Check ControlZone and Linux cluster for clean and idle state.
.br
2. Terminate ControlZone platform processes.
.br
 # su - mzadmin -c "mzsh kill platform"
.br
3. Wait for the cluster to recover from resource failure.
.br
4. Clean up resource fail-count.
.br
5. Check ControlZone and Linux cluster for clean and idle state.
.RE
.PP
.\"
.SH FILES
.\"
.TP
/usr/lib/ocf/resource.d/suse/SAPCMControlZone
the resource agent
.TP
$HOME/.bashrc, e.g. /home/mzadmin/.bashrc
the mzadmin´s .bashrc, defining JAVA_HOME and MZ_HOME
.TP
$MZ_HOME, e.g. /opt/cm/
the installation directory of a CM ControlZone service
.TP
$MZ_HOME/bin/mzsh
the default mzshell, used as API for managing CM ControlZone services, contains paths and URL
.TP
$MZ_HOME/log/
path to logfiles of mzsh as well as platform and UI
.TP
$MZ_HOME/tmp/
temporary files and lock files of platform and UI
.TP
$JAVA_HOME
the JAVA virtual machine, used by mzsh
.\" see https://infozone.atlassian.net/wiki/spaces/MD9/pages/4863840/Terminology
.\" TODO logs?
.PP
.\"
.SH REQUIREMENTS
.\"
* Convergent Mediation ControlZone version 9.0.0.0 or higher is installed and
configured on both cluster nodes. Either the software is installed once into a
shared NFS filesystem and then binaries and configuration are copied into both
cluster nodes´ local filesystems. Or the software is installed per node directly.
However, finally the local configuration has to be adjusted. Please refer to
Convergent Mediation documentation for details.
.PP
* CM ControlZone is configured identically on both cluster nodes. User, path
names and environment settings are the same.
.PP
* Only one ControlZone instance per Linux cluster.
.PP
* Linux shell of the mzadmin user is /bin/bash.
.PP
* The mzadmin´s .bashrc inherits MZ_HOME and JAVA_HOME from SAPCMControlZone RA.
.PP
* When called by the resource agent, mzsh connnects to CM ControlZone services at
localhost. The service´s virtual hostname or virtual IP address managed by the
cluster must never be used when called by RA. 
.PP
* Technical users and groups are defined locally in the Linux system. If users
are resolved by remote service, local caching is neccessary. Substitute user
(su) to the mz-user needs to work reliable and without customized actions or
messages.
.PP
* Name resolution for hostnames and virtual hostnames is crucial. Hostnames of
cluster nodes and services are resolved locally in the Linux system.
.PP
* Strict time synchronization between the cluster nodes, e.g. NTP. All nodes of
a cluster have configured the same timezone.
.PP
* Needed NFS shares (e.g. /usr/sap/<SID>) are mounted statically or by automounter.
No client-side write caching. File locking might be configured for application
needs.
.PP
* The RA monitoring operations have to be active.
.PP
* RA runtime almost completely depends on call-outs to controlled resources,
OS and Linux cluster. The infrastructure needs to allow these call-outs to
return in time.
.PP
* The ControlZone application is not started/stopped by OS. Thus there is no
SystemV, systemd or cron job.
.PP
* As long as the ControlZone application is managed by the Linux cluster, the
application is not started/stopped/moved from outside. Thus no manual actions
are done. The Linux cluster does not prevent from administrative mistakes.
However, if the Linux cluster detects the application running at both sites in
parallel, it will stop both and restart one.
.PP
* Interface for the RA to the ControlZone services is the command mzsh. Ideally,
the mzsh should be accessed on the cluster nodes´ local filesystems.
The mzsh is called with the arguments startup, shutdown, status and kill. Its
output is parsed by the RA. Thus the command and its output needs to be stable.
.PP
* The mzsh is called on the active node with a defined interval for regular
resource monitor operations. It also is called on the active or passive node in
certain situations. Those calls might run in parallel.
.PP
.\"
.SH BUGS
.\"
In case of any problem, please use your favourite SAP support process to open a
request for the component BC-OP-LNX-SUSE.
.br
Please report feedback and suggestions to feedback@suse.com.
.PP
.\"
.SH SEE ALSO
.\"
\fBSAPCMControlZone_basic_cluster\fP(7),
\fBSAPCMControlZone_maintenance_examples\fP(7),
\fBocf_heartbeat_IPaddr2\fP(7) , \fBocf_heartbeat_Filesystem\fP(7) ,
\fBcrm\fP(8) , \fBcrm_mon\fP(8) ,
\fBnfs\fP(5) , \fBmount\fP(8) ,
.br
http://clusterlabs.org/doc/en-US/Pacemaker/1.1/html/Pacemaker_Explained/s-ocf-return-codes.html ,
.br
https://infozone.atlassian.net/wiki/spaces/MD9/pages/4881672/mzsh ,
.br
https://infozone.atlassian.net/wiki/spaces/MD9/pages/4849693/Setting+Environment+Variables+for+Platform ,
.br
https://documentation.suse.com/sbp/sap/ ,
.br
https://documentation.suse.com/#sle-ha ,
.br
https://www.suse.com/support/kb/doc/?id=000019138 ,
.br
https://www.suse.com/support/kb/doc/?id=000019514 ,
.br
https://www.suse.com/support/kb/doc/?id=000019722 ,
.br
https://launchpad.support.sap.com/#/notes/1552925 ,
.br
https://launchpad.support.sap.com/#/notes/3079845
.PP
.\"
.SH AUTHORS
.\"
F.Herschel, L.Pinne
.PP
.\"
.SH COPYRIGHT
.\"
(c) 2023-2024 SUSE LLC
.br
SAPCMControlZone comes with ABSOLUTELY NO WARRANTY.
.br
For details see the GNU General Public License at
http://www.gnu.org/licenses/gpl.html
.\"
