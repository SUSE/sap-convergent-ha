.\" Version: 0.1
.\"
.TH ocf_suse_SAPCMControlZone 7 "23 Nov 2023" "" "SAPCMControlZone"
.\"
.SH NAME
.\"
SAPCMControlZone \- Manages the Convergent Mediation ControlZone platform for a single instance as HA resource.
.PP
.\"
.SH SYNOPSYS
.\"
\fBSAPCMControlZone\fP [ start | stop | monitor | meta\-data | validate\-all | methods | reload | usage ]
.PP
.\"
.SH DESCRIPTION
.\"
SAPCMControlZone is a resource agent for managing the Convergent Mediation (CM)
ControlZone platform and ui for a single instance as HA resources. 
.PP
The CM central ControlZone platform is responsible for providing services to
other instances. Several platform containers may exist in a CM system, for high
availability, but only one is active at a time.
.\" see https://infozone.atlassian.net/wiki/spaces/MD9/pages/4863840/Terminology
The CM central ControlZone UI is used to query, edit, import, and export data.
.\" see https://infozone.atlassian.net/wiki/spaces/MD83/pages/5966420/3.+Web+UI
This ControlZone services can be handled as active/passive resources.
.PP
NFS shares with work directories can be mounted statically on all nodes. The
HA cluster does not need to control that filesystems.
.PP
The resource agent uses the following interface provided by the ControlZone
component:
.PP
mzsh startup \fISERVICE\fP
.PP
mzsh status \fISERVICE\fP
.PP
mzsh shutdown \fISERVICE\fP
.PP
mzsh kill \fISERVICE\fP
.PP
Currently supported services are "platform" and "ui".
.\" TODO output
.PP
Please see also the REQUIREMENTS section below.
.PP
.\"
.SH SUPPORTED PARAMETERS
.\"
This resource agent supports the following parameters:
.PP
\fBUSER\fP
.RS 4
OS user who calls mzsh, owner of $MZ_HOME.
.br
Optional. Unique, string. Default value: "mzadmin".
.RE
.PP
\fBSERVICE\fP
.RS 4
The ControlZone service to be managed by the resoure agent.
.br
Optional. Unique, [ platform | ui ]. Default value: "platform".
.RE
.PP
\fBMZSHELL\fP
.RS 4
Path to mzsh.
.br
.\" TODO /opt/mz/bin/mzsh
Optional. Unique, string. Default value: "/usr/bin/mzsh".
.RE
.PP
\fBCALL_TIMEOUT\fP
.RS 4
Define timeout how long calls to the ControlZone platform for checking the
status can take. If you increase this timeout for ControllZone calls you should 
also adjust the operation timeouts of your Linux cluster resources.
.br
Optional. Unique, integer. Default value: 60.
.RE
.PP
\fBSHUTDOWN_RETRIES\fP
.RS 4
Number of retries to check for process shutdown. Passed to mzsh.
If you increase the number of shutdown retries you should also adjust the stop
operation timeouts of your Linux cluster resources.
.br
Optional. Unique, integer. Default value: 20.
.RE
.PP
\fBVERBOSE_STATUS\fP
.RS 4
Enables verbose mode. Passed to mzsh.
.br
Optional. Unique, [ yes | no ]. Default value: no.
.RE
.PP

.PP
.\"
.SH SUPPORTED ACTIONS
.\"
This resource agent supports the following actions (operations):
.PP
\fBstart\fR
.RS 4
Starts the ControlZone platform resource.
.br
Suggested minimum timeout: 120\&.
.RE
.PP
\fBstop\fR
.RS 4
Stops the ControlZone platform resource.
.br
Suggested minimum timeout: 120\&.
.RE
.PP
\fBmonitor\fR
.RS 4
TODO
Might be adapted to be greater than expected infrastructure timeouts.
.br
Suggested minimum timeout: 120\&.
.RE
.PP
\fBvalidate\-all\fR
.RS 4
Performs a validation of the resource configuration.
.br
Suggested minimum timeout: 5\&.
.RE
.PP
\fBmeta\-data\fR
.RS 4
Retrieves resource agent metadata (internal use only).
.br
Suggested minimum timeout: 5\&.
.RE
.PP
\fBmethods\fR
.RS 4
Reports on the methods, the RA supports.
.br
Suggested minimum timeout: 5\&.
.RE
.PP
\fBreload\fR
.RS 4
Change parameters without forcing a recover of the resource.
.br
Suggested minimum timeout: 5\&.
.RE
.PP
.\"
.SH RETURN CODES
.\"
The return codes are defined by the OCF cluster framework. Please refer to the
OCF definition on the website mentioned below.
.RE
.PP
.\"
.SH EXAMPLES
.\"
Configuration and basic checks for ControlZone platform resources in Linux clusters.
See also man page SAPCMControlZone_maintenance_examples(7).
.PP
\fB* Example configuration for resource group with ControlZone and IP address.\fR
.PP
A ControlZone resoure rsc_cz_C11 is configured, handled by OS user c11adm.
This resource is grouped with an IP address resource rsc_ip_C11 into group
grp_C11. The IP address starts first. The resource group might run on either
node, but never in parallel.
.PP
In case of ControlZone failure (or monitor timeout), the resource group gets
restarted until it gains success or migration-threshold is reached. If
migration-threshold is exceeded, or if the node where the group runs fails, the
group will be moved to the other node.
A priority is configured for correct fencing in split-brain situations.
See also SAPCMControlZone_basic_cluster(7) and ocf_heartbeat_IPAddr2(7).
.PP
.RS 2
primitive rsc_cz_C11 ocf:suse:SAPCMControlZone \\
.br
 params MZ_USER=c11adm \\
.br
 op monitor interval=120 timeout=120 on-fail=restart \\
.br
 op start timeout=120 \\
.br
 op stop timeout=120 \\
.br
 meta priority=100
.RE
.PP
.RS 2
primitive rsc_ip_C11 ocf:heartbeat:IPaddr2 \\
.br
 params ip=192.168.1.234 \\
.br
 op monitor interval=60 timeout=20 on-fail=restart
.RE
.PP
.RS 2
group grp_C11 \\
.br
 rsc_ip_C11 rsc_cz_C11
.PP
.RE
\fB* Optional Filesystem resource for monitoring NFS shares.\fR
.PP
A shared filesystem migth be statically mounted by OS on both cluster nodes.
This filesystem holds work directories. It must not be confused with the
ControlZone application itself. Client-side write caching has to be disabled.
.PP
A Filesystem resource is configured for a bind-mount of the real NFS share.
This resource is grouped with the ControlZone platform and IP address. In case
of filesystem failures, the whole group gets restarted.
No mount or umount on the real NFS share is done.
Example for the real NFS share is /mnt/platform/check/, example for the
bind-mount is /mnt/check/. Both mount points have to be created before the
cluster resource is activated. 
See also man page SAPCMControlZone_basic_cluster(7), ocf_heartbeat_Filesystem(7)
and nfs(5).
.PP
.RS 2
primitive rsc_fs_C11 ocf:heartbeat:Filesystem \\
.br
 params device=/mnt/platform/check/ directory=/mnt/check/ \\
.br
 fstype=nfs4 options=bind,rw,noac,sync,defaults \\
.br
 op monitor interval=120 timeout=120 on-fail=restart \\
.br
 op_params OCF_CHECK_LEVEL=20 \\
.br
 op start timeout=120 \\
.br
 op stop timeout=120
.RE
.PP
.RS 2
group grp_C11 \\
.br
 rsc_fs_C11 rsc_ip_C11 rsc_cz_C11
.RE
.PP
\fB* Show configuration of ControlZone platform resource and resource group.\fR
.PP
Resource is rsc_cz_C11, resource group is grp_C11.
.PP
.RS 2 
# crm configure show rsc_cz_C11 grp_C11
.RE
.PP
\fB* Search for log entries of SAPCMControlZone, show errors only.\fR
.PP
.RS 2
# grep "SAPCMControlZone.*RA.*rc=[1-7,9]" /var/log/messages
.RE
.PP
\fB* Show failcount for resource rsc_cz_C11.\fR
.PP
.RS 2
# cibadmin -Ql | grep rsc_cz_C1.*fail-count
.RE
.PP
\fB* Manually trigger a SAPCMControlZone probe action.\fR
.PP
.RS 2
# OCF_ROOT=/usr/lib/ocf/ \\
.br
OCF_RESKEY_CRM_meta_interval=0 \\
.br
/usr/lib/ocf/resource.d/suse/SAPCMControlZone monitor
.RE
.PP
\fB* Example for testing the SAPCMControlZone RA.\fR
.PP
The ControlZone platform will be terminated, while controlled by the
Linux cluster. This could be done as very basic testing of SAPCMControlZone RA
integration. Terminating ControlZone platform processes is dangerous. This test
should not be done on production systems. Example user is mzadmin.
.br
Note: Understand the impact before trying.
.PP
.RS 2
1. Check ControlZone and Linux cluster for clean and idle state.
.br
2. Terminate ControlZone platform processes.
.br
 # su - mzadmin -c "mzsh kill platform"
.br
3. Wait for the cluster to recover from resource failure.
.br
4. Clean up resource fail-count.
.br
5. Check ControlZone and Linux cluster for clean and idle state.
.RE
.PP
.\"
.SH FILES
.\"
.TP
/usr/lib/ocf/resource.d/suse/SAPCMControlZone
the resource agent
.TP
$MZ_HOME, e.g. /opt/mz/
the installation directory of a ControlZone service
.TP
$MZ_HOME/bin/mzsh
the mzshell, used as API for managing ControlZone components
.TP
$MZ_HOME/tmp/
temporary files of a ControlZone service
.\" see https://infozone.atlassian.net/wiki/spaces/MD9/pages/4863840/Terminology
.PP
.\"
.SH REQUIREMENTS
.\"
* Convergent Mediation ControlZone version 9.0.0.0 or higher is installed and
configured on both cluster nodes. If the software is installed into a shared NFS
filesystem, the binaries are copied into both cluster nodes´ local filesystems. 
.PP
* Only one ControlZone instance per Linux cluster.
.PP
* Technical users and groups are defined locally in the Linux system. If users
are resolved by remote service, local caching is neccessary. Substitute user
(su) to the mz-user (e.g. "mzadmin") needs to work reliable and without
customized actions or messages.
.PP
* Strict time synchronization between the cluster nodes, e.g. NTP. All nodes of
a cluster have configured the same timezone.
.PP
* Needed NFS shares (e.g. /mnt/platform/) mounted statically or by automounter. 
No client-side write caching.
.PP
* The RA monitoring operations have to be active.
.PP
* RA runtime almost completely depends on call-outs to controlled resources,
OS and Linux cluster. The infrastructure needs to allow these call-outs to
return in time.
.PP
* The ControlZone application is not started/stopped by OS. Thus there is no
SystemV, systemd or cron job.
.PP
* As long as the ControlZone application is managed by the Linux cluster, the
application is not started/stopped/moved from outside. Thus no manual actions
are done.
.PP
* Interface for the RA to the ControlZone platform is the command mzsh. The
mzsh is accessed on the cluster nodes´ local filesystems.
The mzsh is called with the arguments startup, shutdown, status and kill. Its
output is parsed by the RA. Thus the command and its output needs to be stable.
.PP
* The mzsh is called on the active node with a defined interval for regular
resource monitor operations. It also is called on the active or passive node in
certain situations. Those calls might run in parallel.
.PP
.\"
.SH BUGS
.\"
In case of any problem, please use your favourite SAP support process to open a
request for the component BC-OP-LNX-SUSE.
.br
Please report feedback and suggestions to feedback@suse.com.
.PP
.\"
.SH SEE ALSO
.\"
\fBSAPCMControlZone_basic_cluster\fP(7),
\fBSAPCMControlZone_maintenance_examples\fP(7),
\fBocf_heartbeat_IPaddr2\fP(7) , \fBocf_heartbeat_Filesystem\fP(7) ,
\fBcrm\fP(8) , \fBcrm_mon\fP(8) ,
\fBnfs\fP(5) , \fBmount\fP(8) ,
.br
http://clusterlabs.org/doc/en-US/Pacemaker/1.1/html/Pacemaker_Explained/s-ocf-return-codes.html ,
.br
https://infozone.atlassian.net/wiki/spaces/MD9/pages/4881672/mzsh ,
.br
https://documentation.suse.com/sbp/sap/ ,
.br
https://documentation.suse.com/#sle-ha ,
.br
https://www.suse.com/support/kb/doc/?id=000019722 ,
.br
https://launchpad.support.sap.com/#/notes/1552925 ,
.br
https://launchpad.support.sap.com/#/notes/3079845
.PP
.\"
.SH AUTHORS
.\"
F.Herschel, L.Pinne
.PP
.\"
.SH COPYRIGHT
.\"
(c) 2023 SUSE LLC
.br
SAPCMControlZone comes with ABSOLUTELY NO WARRANTY.
.br
For details see the GNU General Public License at
http://www.gnu.org/licenses/gpl.html
.\"
