.\ Version: 0.1
.\"
.TH SAPCMControlZone_maintenance_examples 7 "15 Apr 2024" "" "SAPCMControlZone"
.\"
.SH NAME
.\"
SAPCMControlZone_maintenance_examples \- maintenance examples for SAPCMControlZone.
.PP
.\"
.SH DESCRIPTION
.\"
Maintenance examples for ControlZone resources in Linux HA clusters. Please see
ocf_suse_SAPCMControlZone(7) for more examples and read the REQUIREMENTS
section there.
.PP
.\"
.SH EXAMPLES
.\"
\fB* Example for checking clean and idle state.\fR
.PP
This steps should be performed before doing anything with the cluster, and
after something has been done.
Example user is mzadmin.
.PP
.RS 2 
# su - mzadmin -c "mzsh status"
.br
# crm_mon -1r
.br
# crm configure show | grep cli-
.br
# cibadmin -Q | grep fail-count
.br
# cs_clusterstate -i
.RE
.PP
\fB* Example for manual takeover using Linux cluster.\fR
.PP
ControlZone application and Linux cluster are checked for clean and idle state.
The ControlZone resources are moved to the other node. The related location rule
is removed after the takeover took place.
ControlZone application and Linux cluster are checked for clean and idle state.
Resource group is grp_C11, user is mzadmin.
.PP
.RS 2
# su - mzadmin -c "mzsh status"
.br
# crm_mon -1r
.br
# crm configure show | grep cli-
.br
# cibadmin -Q | grep fail-count
.br
# cs_clusterstate -i
.PP
# crm resource move grp_C11 force
.br
# cs_wait_for_idle -s 9; crm_mon -1r
.br
# crm resource clear grp_C11
.PP
# cs_wait_for_idle -s 6; crm_mon -1r
.br
# crm configure show | grep cli-
.br
# su - mzadmin -c "mzsh status"
.RE
.PP
\fB* Example for generic maintenance procedure.\fR
.PP
ControlZone application and Linux cluster are checked for clean and idle state.
The ControlZone resource group is set into maintenance mode.
This is needed to allow manual actions on the resources.
After the manual actions are done, the resource group is put back under cluster
control. It is neccessary to wait for each step to complete and to check the
result.
ControlZone application and Linux cluster are checked for clean and idle state.
Resource group is grp_C11, user is mzadmin. See also example above.
.PP
.RS 2
# su - mzadmin -c "mzsh status"
.br
# crm_mon -1r
.br
# crm configure show | grep cli-
.br
# crm configure show | grep cli-
.br
# cs_clusterstate -i
.br
# crm resource maintenance grp_C11
.PP
<do maintenance>
.PP
# crm resource refresh grp_C11
.br
# cs_wait_for_idle -s 6; crm_mon -1r
.br
# crm resource maintenance grp_C11 off
.br
# cs_wait_for_idle -s 6; crm_mon -1r
.br
# su - mzadmin -c "mzsh status"
.RE
.PP
\fB* Set whole Linux cluster into maintenance.\fR
.PP
This disables all resource management as well as node fencing. However, fence
requests from outside will be queued and executed once the cluster leaves
maintenance mode. See manual page crm(8) and stonith_admin(8).
.PP
.RS 2
# crm maintenance on
.br
# crm_attribute --query -t crm_config -n maintenance-mode
.RE
.PP
\fB* Remove left-over maintenance attribute from overall Linux cluster.\fR
.PP
This could be done to avoid confusion caused by different maintenance procedures.
Before doing so, check for cluster attribute maintenance-mode="false".
.PP
.RS 2
# crm_attribute --query -t crm_config -n maintenance-mode
.br
# crm_attribute --delete -t crm_config -n maintenance-mode
.RE
.PP
\fB* Remove left-over standby attribute from Linux cluster nodes.\fR
.PP
This could be done to avoid confusion caused by different maintenance procedures.
Before doing so for all nodes, check for node attribute standby="off" on all
nodes. Example node is node1.
.PP
.RS 2 
# crm_attribute --query -t nodes -N node1 -n standby
.br
# crm_attribute --delete -t nodes -N node1 -n standby
.RE
.PP
\fB* Remove left-over maintenance attribute from resource.\fR
.PP
This should usually not be needed.
Resource group is grp_C11.
.PP
.RS 2
# crm_resource --resource grp_C11 --delete-parameter maintenance --meta
.\" .br
.\" # TODO check
.RE
.PP
\fB* Disable Linux cluster on all cluster nodes.\fR
.PP
On any cluster node the cluster will not start automatically on boot anymore.
Nevertheless a currently running cluster will keep running.
Needs password-less ssh between cluster nodes.
.PP
.RS 2
# crm cluster run "crm cluster disable"
.br
# crm cluster run "systemctl status pacemaker" | grep pacemaker.service
.RE
.PP
\fB* Start Linux cluster on all cluster nodes.\fR
.PP
Needs password-less ssh between cluster nodes.
.PP
.RS 2
# crm cluster start --all
.br
# crm cluster run "systemctl status pacemaker" | grep pacemaker.service
.RE
.PP
\fB* Overview  on maintenance procedure for Linux cluster or OS.\fR
.PP
ControlZone instance remains running. But it will not automatically be moved or
restarted while the Linux cluster is inactive. It is neccessary to wait for
each step to complete and to check the result. See examples above for details.
.PP
.RS 2
1. Check status of Linux cluster and ControlZone instance, see above.
.br
2. Set the Linux cluster into maintenance mode.
.br
3. Stop Linux Cluster on all nodes.
.br
4. Perform maintenance on Linux cluster or OS.
.br
5. Start Linux cluster on all nodes.
.br
6. Let Linux cluster detect status of ControlZone resources.
.br
7. Set cluster ready for operations.
.br
8. Check status of Linux cluster and ControlZone instance, see above.
.RE
.PP
.\"
.SH BUGS
.\"
Please report feedback and suggestions to feedback@suse.com.
.PP
.\"
.SH SEE ALSO
.\"
\fBocf_suse_SAPCMControlZone\fP(7) , \fBSAPCMControlZone_basic_cluster\fP(7) ,
\fBcrm\fP(8) , \fBcrm_simulate\fP(8) , \fBcrm_report\fP(8) , \fBcibadmin\fP(8) ,
\fBsbd\fP(8) , \fBstonith_admin\fP(8) , \fBcorosync-cfgtool\fP(8) ,
\fBcs_clusterstate\fP(8) , \fBcs_wait_for_idle\fP(8) , 
\fBha_related_sap_notes\fP(7) , \fBha_related_suse_tids\fP(7)
.PP
.\"
.SH AUTHORS
.\"
F.Herschel, L.Pinne
.PP
.\"
.SH COPYRIGHT
.\"
(c) 2024 SUSE LLC
.br
SAPCMControlZone comes with ABSOLUTELY NO WARRANTY.
.br
For details see the GNU General Public License at
http://www.gnu.org/licenses/gpl.html
.\"
